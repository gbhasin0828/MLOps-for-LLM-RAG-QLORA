# -*- coding: utf-8 -*-
"""MLflow_LLM_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AogunMwma5KfP6K_1UzVD_xLzp_2a-Bf
"""

from google.colab import files
uploaded = files.upload()

pip install pandas numpy transformers pinecone-client torch scikit-learn

pip install mlflow

from transformers import AutoTokenizer, AutoModel



import rag_model_semantic_search as rag

from rag_model_semantic_search import generate_query_embedding, retrieve_from_pinecone, generate_response_with_model

import mlflow
from time import time

mlflow.set_experiment("RAG_Model_Experiments")

def test_in_mlflow(query, top_k = 5):
  with mlflow.start_run(run_name="Retrival-Only"):
    start_time = time()
    context = retrieve_from_pinecone(query, top_k=5)
    retrival_time = time() - start_time
    mlflow.log_param("method", "retrival-only")
    mlflow.log_param("Top-K", top_k)
    mlflow.log_metric("retrival-time", retrival_time)
    mlflow.log_text(f"Query: {query}\nContext: {context}", "retrieval_results.txt")
    print(f"Logged retrieval-only workflow with time: {retrieval_time:.2f}s")


    # Generative Workflows
  for model_name in rag.generative_models.keys():
    with mlflow.start_run(run_name=f"Generative Model {model_name}"):
      start_time = time()
      response = generate_response_with_model(query, context, model_name)
      response_time = time() - start_time
      mlflow.log_param("method", "retrival+generative")
      mlflow.log_param("model-name", model_name)
      mlflow.log_param("Top-K", top_k)
      mlflow.log_metric("response-time", response_time)
      mlflow.log_text(f"Query: {query}\nContext: {context}\nResponse: {response}", "response_results.txt")
      print(f"Logged {model_name} workflow with time: {response_time:.2f}s")

query = "Give me top 5 promotions for item Item_177 at customer Discount"

test_in_mlflow(query, top_k = 5)