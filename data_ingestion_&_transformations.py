# -*- coding: utf-8 -*-
"""Data Ingestion & Transformations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9rvARvj5KGGunfSkDNVar1VysB05X88

# **Data Ingestion & Transformations**
"""

pip install boto3 pandas pyarrow s3fs pyspark

S3_Bucket = "Trade-Promotions-Performance-Data"
Raw_Data_Key = "raw_data/Trade_Data_Raw.csv"
Processed_Data_key = "processed_data/Trade_Data_Processed.csv"

import boto3
from botocore.exceptions import NoCredentialsError
import pandas as pd

def download_data_from_s3(S3_Bucket, Raw_Data_Key):
    s3 = boto3.client('s3')
    try:
        s3.download_file(S3_Bucket, Raw_Data_Key, 'Trade_Data_Raw.csv')

data_file = download_data_from_s3(S3_Bucket, Raw_Data_Key)

df = pd.read_csv('data_file')

def transform_data(df):
    """
    Perform data transformations to add derived fields based on provided formulas.
    """
    print("Transforming data...")
    try:
        # Apply derived formulas
        df['Margin_%'] = df.apply(
            lambda row: (row['Base_Price'] - row['List_Price'] + row['EDLP_Trade_Unit']) / row['Base_Price']
            if row['Week_Type'] == 'Base'
            else (row['Promo_Price'] - row['List_Price'] + row['EDLP_Trade_Unit'] + row['Var_Trade_Unit']) / row['Promo_Price'],
            axis=1
        )

        df['Trade_Unit'] = df.apply(
            lambda row: row['EDLP_Trade_Unit']
            if row['Week_Type'] == 'Base'
            else row['EDLP_Trade_Unit'] + row['Var_Trade_Unit'],
            axis=1
        )

        df['Trade_Rate'] = df.apply(
            lambda row: row['Trade_Unit'] / row['List_Price'],
            axis=1
        )

        df['Profit_Unit'] = df.apply(
            lambda row: row['List_Price'] - row['COGS_Unit'] - row['EDLP_Trade_Unit']
            if row['Week_Type'] == 'Base'
            else row['List_Price'] - row['COGS_Unit'] - (row['EDLP_Trade_Unit'] + row['Var_Trade_Unit']),
            axis=1
        )

        df['Profit_Unit_Percentage'] = df.apply(
            lambda row: row['Profit_Unit'] / row['List_Price'],
            axis=1
        )

        df['Lift_%'] = df.apply(
            lambda row: (row['Promo_Units'] - row['Base_Units']) / row['Base_Units']
            if row['Week_Type'] == 'Base'
            else 0,
            axis=1
        )

        df['Inc_Profit'] = df.apply(
            lambda row: (row['Promo_Units'] * (row['List_Price'] - row['COGS_Unit'] - row['EDLP_Trade_Unit'] - row['Var_Trade_Unit']))
            - (row['Base_Units'] * (row['List_Price'] - row['COGS_Unit'] - row['EDLP_Trade_Unit']))
            if row['Promo_Units'] > 0
            else 0,
            axis=1
        )

        df['ROI'] = df.apply(
            lambda row: row['Inc_Profit'] / (row['Promo_Units'] * row['Var_Trade_Unit'])
            if row['Var_Trade_Unit'] > 0
            else 0,
            axis=1
        )

        df['Discount'] = df.apply(
            lambda row: (row['Base_Price'] - row['Promo_Price']) / row['Base_Price'] * 100
            if row['Base_Price'] > 0
            else 0,
            axis=1
        )

        print("Data transformation completed successfully!")
        return df

    except Exception as e:
        print(f"Error during transformation: {e}")
        return None

df_transformer = transform_data(df)

def save_data_locally(df, output_path):
    """Save the processed DataFrame to a local file."""
    try:
        df.to_csv(output_path, index=False)
        print(f"Processed data saved locally to {output_path}")
    except Exception as e:
        print(f"Failed to save processed data: {e}")

# Save locally
save_data_locally(df_transformed, LOCAL_PROCESSED_PATH)

"""# **Create a full Data Pipeline**"""

def run_data_pipeline(S3_Bucket, Raw_Data_Key):

  data_file = download_data_from_s3(S3_Bucket, Raw_Data_Key)

  df_raw = pd.read_csv(data_file)

  df_transformed = transform_data(df_raw)

  save_data_locally(df_transformed, LOCAL_PROCESSED_PATH)